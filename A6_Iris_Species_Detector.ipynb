{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A6_Iris_Species_Detector.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM2u3lJ5MSor1RqZIthjgAH"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_Hka-ooRtG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##This block is only for access of files using google drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#For accessing any file from google drive, first share it for public access. Copy its id from last part of its address. Then specify the two lines below.\n",
        "downloaded = drive.CreateFile({'id':\"120DXm92IJY_3NNRHpakD-q7jUCerg43E\"})   # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Iris.csv')        # replace the file name with your file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fShgaARjSukD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random;\n",
        "import numpy as np;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBsl3HT-Louh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#read File\n",
        "I = pd.read_csv('Iris.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg7A3OB9X6FO",
        "colab_type": "text"
      },
      "source": [
        "The Iris dataset was used in R.A. Fisher's classic 1936 paper, The Use of Multiple Measurements in Taxonomic Problems, and can also be found on the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Iris).\n",
        "\n",
        "It includes three iris species with 50 samples each as well as some properties about each flower. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n",
        "\n",
        "The columns in this dataset are:\n",
        "\n",
        "\n",
        "1.   Id\n",
        "2. SepalLengthCm\n",
        "3. SepalWidthCm\n",
        "4. PetalLengthCm\n",
        "5. PetalWidthCm\n",
        "6. Species\n",
        "\n",
        "\n",
        "\n",
        "#Assignment 6 Multi-class classification and K-fold cross-validation (4)\n",
        "\n",
        "1.   Code the Species as 0, 1, 2\n",
        "2.   Shuffle the data\n",
        "3.   Separate it into 10 equal sets\n",
        "4.   Combine 7 sets as training set and 1 set as validation set and 2 sets as test set.\n",
        "5.   Normalize the training set\n",
        "6.   Write a code for multi-class logistic regression\n",
        "7.   Test the performance on the validation set.\n",
        "8.   Repat from step 4 to 7 eight times for all the 8 possible validation sets sets each time with a different validation set.\n",
        "9.   Report the mean and standard deviation of the 8 cross-validation accuracies. Report the accuracy on the test-set. Use decision threshold of 0.5.\n",
        "10. For the three classifiers that you have designed, construct the confusion matrix for 0.5 threshold and the ROC curve by changing the decision threshold from 0 to 1 with a step of 0.1. Which thresholds appear best for the three classifiers?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXCR7g-CcY-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Code the Species as 0, 1, 2\n",
        "I['Species'].replace('Iris-setosa',0,inplace=True)\n",
        "I['Species'].replace('Iris-versicolor',1,inplace=True)\n",
        "I['Species'].replace('Iris-virginica',2,inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FE_8j5QUSNeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Shuffle the data\n",
        "from sklearn.utils import shuffle\n",
        "I=shuffle(I)\n",
        "I=I.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_c894wtWSdHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Separate it into 10 equal sets\n",
        "Train_set_1=I[0:15]\n",
        "Train_set_2=I[15:30]\n",
        "Train_set_3=I[30:45]\n",
        "Train_set_4=I[45:60]\n",
        "Train_set_5=I[60:75]\n",
        "Train_set_6=I[75:90]\n",
        "Train_set_7=I[90:105]\n",
        "Train_set_8=I[105:120]\n",
        "Train_set_9=I[120:135]\n",
        "Train_set_10=I[135:150]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0G8Sr9oSgCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Combine 7 sets as training set and 1 set as validation set and 2 sets as test set.\n",
        "\n",
        "Train_set=Train_set_1.append(Train_set_2).append(Train_set_3).append(Train_set_4).append(Train_set_5).append(Train_set_6).append(Train_set_7)\n",
        "Valid_set=Train_set_8\n",
        "Test_set=Train_set_9.append(Train_set_10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqqt1vMzvM9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training sets, validation set and Test sets\n",
        "Train_1=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_1=Train_set_8;\n",
        "Test_1=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_2=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_10])\n",
        "Valid_2=Train_set_7;\n",
        "Test_2=pd.concat([Train_set_9,Train_set_8])\n",
        "\n",
        "Train_3=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_8, Train_set_7])\n",
        "Valid_3=Train_set_6;\n",
        "Test_3=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_4=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_8, Train_set_6, Train_set_7])\n",
        "Valid_4=Train_set_5;\n",
        "Test_4=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_5=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_8, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_5=Train_set_4;\n",
        "Test_5=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_6=pd.concat([Train_set_1, Train_set_2, Train_set_8, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_6=Train_set_3;\n",
        "Test_6=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_7=pd.concat([Train_set_1, Train_set_8, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_7=Train_set_2;\n",
        "Test_7=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_8=pd.concat([Train_set_8, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_8=Train_set_1;\n",
        "Test_8=pd.concat([Train_set_9,Train_set_10])\n",
        "\n",
        "Train_9=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_9=Train_set_9;\n",
        "Test_9=pd.concat([Train_set_8,Train_set_10])\n",
        "\n",
        "Train_10=pd.concat([Train_set_1, Train_set_2, Train_set_3, Train_set_4, Train_set_5, Train_set_6, Train_set_7])\n",
        "Valid_10=Train_set_10;\n",
        "Test_10=pd.concat([Train_set_9,Train_set_8])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjWSkTqgSnTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#sigmod function\n",
        "def sigmoid(x):\n",
        "  z = 1/(1 + np.exp(-x))\n",
        "  return z;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0p48SVrm4k8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#function to find accuracy for valiadation set and test set\n",
        "def Acc_Iris(Train_set,Test_set,Valid_set):\n",
        "  T1=T2=T3=Train_set;\n",
        "\n",
        "  #find faverarble prob. of 0\n",
        "\n",
        "  #replacing 1,2 with 0 and 0 with 1\n",
        "  T1['Species'].replace(0,3,inplace=True)\n",
        "  T1['Species'].replace(1,0,inplace=True)\n",
        "  T1['Species'].replace(2,0,inplace=True)\n",
        "  T1['Species'].replace(3,1,inplace=True)\n",
        "\n",
        "  #Normalize the training set\n",
        "\n",
        "  X1 = T1['SepalLengthCm'];\n",
        "  X1_Mean = np.mean(X1);\n",
        "  X1_Std = np.std(X1);\n",
        "  X1_N = (X1-np.mean(X1))/np.std(X1);\n",
        "\n",
        "  X2 = T1['SepalWidthCm'];\n",
        "  X2_Mean = np.mean(X2);\n",
        "  X2_Std = np.std(X2);\n",
        "  X2_N = (X2-np.mean(X2))/np.std(X2);\n",
        "\n",
        "  X3 = T1['PetalLengthCm'];\n",
        "  X3_Mean = np.mean(X3);\n",
        "  X3_Std = np.std(X3);\n",
        "  X3_N = (X3-np.mean(X3))/np.std(X3);\n",
        "\n",
        "  X4 = T1['PetalWidthCm'];\n",
        "  X4_Mean = np.mean(X4);\n",
        "  X4_Std = np.std(X4);\n",
        "  X4_N = (X4-np.mean(X4))/np.std(X4);\n",
        "\n",
        "  Y=T1['Species']\n",
        "  #random initialization\n",
        "  m = X1.shape[0];\n",
        "  a0 = random.uniform(-1,1);\n",
        "  a1 = random.uniform(-1,1);\n",
        "  a2 = random.uniform(-1,1);\n",
        "  a3 = random.uniform(-1,1);\n",
        "  a4 = random.uniform(-1,1);\n",
        "  learning_Rate = 1;\n",
        "  #Weight updation\n",
        "  for i in range(0,500):\n",
        "      h0 = sigmoid(a0+a1*X1_N+a2*X2_N+a3*X3_N+a4*X4_N);\n",
        "      a0 = a0 - (learning_Rate/m)*np.sum(h0-Y);\n",
        "      a1 = a1 - (learning_Rate/m)*np.sum((h0-Y)*X1_N);\n",
        "      a2 = a2 - (learning_Rate/m)*np.sum((h0-Y)*X2_N);\n",
        "      a3 = a3 - (learning_Rate/m)*np.sum((h0-Y)*X3_N);\n",
        "      a4 = a4 - (learning_Rate/m)*np.sum((h0-Y)*X4_N);\n",
        "\n",
        "  #find faverarble prob. of 1\n",
        "\n",
        "  #replacing 0,2 with 0 \n",
        "  T2['Species'].replace(2,0,inplace=True)\n",
        "\n",
        "  #Normalize the training set\n",
        "\n",
        "  X1 = T2['SepalLengthCm'];\n",
        "  X1_Mean = np.mean(X1);\n",
        "  X1_Std = np.std(X1);\n",
        "  X1_N = (X1-np.mean(X1))/np.std(X1);\n",
        "\n",
        "  X2 = T2['SepalWidthCm'];\n",
        "  X2_Mean = np.mean(X2);\n",
        "  X2_Std = np.std(X2);\n",
        "  X2_N = (X2-np.mean(X2))/np.std(X2);\n",
        "\n",
        "  X3 = T2['PetalLengthCm'];\n",
        "  X3_Mean = np.mean(X3);\n",
        "  X3_Std = np.std(X3);\n",
        "  X3_N = (X3-np.mean(X3))/np.std(X3);\n",
        "\n",
        "  X4 = T2['PetalWidthCm'];\n",
        "  X4_Mean = np.mean(X4);\n",
        "  X4_Std = np.std(X4);\n",
        "  X4_N = (X4-np.mean(X4))/np.std(X4);\n",
        "\n",
        "  Y=T2['Species']\n",
        "\n",
        "  #random initialization\n",
        "\n",
        "  m = X1.shape[0];\n",
        "  aa0 = random.uniform(-1,1);\n",
        "  aa1 = random.uniform(-1,1);\n",
        "  aa2 = random.uniform(-1,1);\n",
        "  aa3 = random.uniform(-1,1);\n",
        "  aa4 = random.uniform(-1,1);\n",
        "  learning_Rate = 1;\n",
        "\n",
        "  #Weight updation\n",
        "\n",
        "  for i in range(0,500):\n",
        "      h1 = sigmoid(a0+a1*X1_N+a2*X2_N+a3*X3_N+a4*X4_N);\n",
        "      aa0 = aa0 - (learning_Rate/m)*np.sum(h1-Y);\n",
        "      aa1 = aa1 - (learning_Rate/m)*np.sum((h1-Y)*X1_N);\n",
        "      aa2 = aa2 - (learning_Rate/m)*np.sum((h1-Y)*X2_N);\n",
        "      aa3 = aa3 - (learning_Rate/m)*np.sum((h1-Y)*X3_N);\n",
        "      aa4 = aa4 - (learning_Rate/m)*np.sum((h1-Y)*X4_N);\n",
        "\n",
        "  #find faverarble prob. of 2\n",
        "\n",
        "  #replacing 0,1 with 0 \n",
        "  T3['Species'].replace(1,0,inplace=True)\n",
        "  T3['Species'].replace(2,1,inplace=True)\n",
        "\n",
        "  #Normalize the training set\n",
        "\n",
        "  X1 = T3['SepalLengthCm'];\n",
        "  X1_Mean = np.mean(X1);\n",
        "  X1_Std = np.std(X1);\n",
        "  X1_N = (X1-np.mean(X1))/np.std(X1);\n",
        "\n",
        "  X2 = T3['SepalWidthCm'];\n",
        "  X2_Mean = np.mean(X2);\n",
        "  X2_Std = np.std(X2);\n",
        "  X2_N = (X2-np.mean(X2))/np.std(X2);\n",
        "\n",
        "  X3 = T3['PetalLengthCm'];\n",
        "  X3_Mean = np.mean(X3);\n",
        "  X3_Std = np.std(X3);\n",
        "  X3_N = (X3-np.mean(X3))/np.std(X3);\n",
        "\n",
        "  X4 = T3['PetalWidthCm'];\n",
        "  X4_Mean = np.mean(X4);\n",
        "  X4_Std = np.std(X4);\n",
        "  X4_N = (X4-np.mean(X4))/np.std(X4);\n",
        "\n",
        "  Y=T3['Species']\n",
        "  #random initialization\n",
        "  m = X1.shape[0];\n",
        "  ab0 = random.uniform(-1,1);\n",
        "  ab1 = random.uniform(-1,1);\n",
        "  ab2 = random.uniform(-1,1);\n",
        "  ab3 = random.uniform(-1,1);\n",
        "  ab4 = random.uniform(-1,1);\n",
        "  learning_Rate = 1;\n",
        "  #Weight updation\n",
        "  for i in range(0,500):\n",
        "      h2 = sigmoid(a0+a1*X1_N+a2*X2_N+a3*X3_N+a4*X4_N);\n",
        "      ab0 = ab0 - (learning_Rate/m)*np.sum(h2-Y);\n",
        "      ab1 = ab1 - (learning_Rate/m)*np.sum((h2-Y)*X1_N);\n",
        "      ab2 = ab2 - (learning_Rate/m)*np.sum((h2-Y)*X2_N);\n",
        "      ab3 = ab3 - (learning_Rate/m)*np.sum((h2-Y)*X3_N);\n",
        "      ab4 = ab4 - (learning_Rate/m)*np.sum((h2-Y)*X4_N);\n",
        "\n",
        "  #Test the performance on the validation set.\n",
        "\n",
        "  Valid_set=Valid_set.reset_index(drop=True);\n",
        "\n",
        "  V1=Valid_set['SepalLengthCm']\n",
        "  V2=Valid_set['SepalWidthCm']\n",
        "  V3=Valid_set['PetalLengthCm']\n",
        "  V4=Valid_set['PetalWidthCm']\n",
        "\n",
        "  #normalized vales\n",
        "  V1_N = (V1 - X1_Mean)/X1_Std\n",
        "  V2_N = (V2 - X2_Mean)/X2_Std\n",
        "  V3_N = (V3 - X3_Mean)/X3_Std\n",
        "  V4_N = (V4 - X4_Mean)/X4_Std\n",
        "\n",
        "  Vh_0 = sigmoid(a0+a1*V1_N+a2*V2_N+a3*V3_N+a4*V4_N)\n",
        "  Vh_1 = sigmoid(aa0+aa1*V1_N+aa2*V2_N+aa3*V3_N+aa4*V4_N)\n",
        "  Vh_2 = sigmoid(ab0+ab1*V1_N+ab2*V2_N+ab3*V3_N+ab4*V4_N)\n",
        "  \n",
        "  #prediction\n",
        "\n",
        "  predict = [None] * len(Valid_set)\n",
        "  for i in range(0,len(Valid_set)):\n",
        "    if (Vh_0[i] >= Vh_1[i] and Vh_0[i] >= Vh_2[i]):\n",
        "      predict[i]=0;\n",
        "    elif (Vh_1[i] >= Vh_0[i] and Vh_1[i] >= Vh_2[i]):\n",
        "      predict[i]=1;\n",
        "    elif (Vh_2[i] >= Vh_0[i] and Vh_2[i] >= Vh_1[i]):\n",
        "      predict[i]=2;\n",
        "\n",
        "  N=0\n",
        "  for i in range(len(Valid_set)):\n",
        "    if predict[i] == Valid_set.Species[i]:\n",
        "      N = N+1\n",
        "  Accuracy_val1 = N/len(Valid_set)\n",
        "\n",
        "  #Test the performance on the Test set.\n",
        "  Test_set=Test_set.reset_index(drop=True);\n",
        "\n",
        "  V1=Test_set['SepalLengthCm']\n",
        "  V2=Test_set['SepalWidthCm']\n",
        "  V3=Test_set['PetalLengthCm']\n",
        "  V4=Test_set['PetalWidthCm']\n",
        "\n",
        "  #normalized vales\n",
        "  V1_N = (V1 - X1_Mean)/X1_Std\n",
        "  V2_N = (V2 - X2_Mean)/X2_Std\n",
        "  V3_N = (V3 - X3_Mean)/X3_Std\n",
        "  V4_N = (V4 - X4_Mean)/X4_Std\n",
        "\n",
        "  Vh_0 = sigmoid(a0+a1*V1_N+a2*V2_N+a3*V3_N+a4*V4_N)\n",
        "  Vh_1 = sigmoid(aa0+aa1*V1_N+aa2*V2_N+aa3*V3_N+aa4*V4_N)\n",
        "  Vh_2 = sigmoid(ab0+ab1*V1_N+ab2*V2_N+ab3*V3_N+ab4*V4_N)\n",
        "\n",
        "  predict = [None] * len(Test_set)\n",
        "  for i in range(len(Test_set)):\n",
        "    if (Vh_0[i] >= Vh_1[i] and Vh_0[i] >= Vh_2[i]):\n",
        "      predict[i]=0;\n",
        "    elif (Vh_1[i] >= Vh_0[i] and Vh_1[i] >= Vh_2[i]):\n",
        "      predict[i]=1;\n",
        "    elif (Vh_2[i] >= Vh_0[i] and Vh_2[i] >= Vh_1[i]):\n",
        "      predict[i]=2;\n",
        "\n",
        "  N=0\n",
        "  for i in range(len(Test_set)):\n",
        "    if predict[i] == Test_set.Species[i]:\n",
        "      N = N+1\n",
        "  Accuracy_val2 = N/len(Test_set)\n",
        "  #print('Accuracy value for Valid set =', Accuracy_val1*100,'%')\n",
        "  #print('Accuracy value for Test set =', Accuracy_val2*100,'%')\n",
        "  return Accuracy_val1,Accuracy_val2;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMlK9rQnudb3",
        "colab_type": "code",
        "outputId": "7eed5e3d-6183-48ad-dd43-1e1493f02ece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#accuracy on different sets\n",
        "a1=Acc_Iris(Train_1,Test_1,Valid_1)\n",
        "a2=Acc_Iris(Train_2,Test_2,Valid_2)\n",
        "a3=Acc_Iris(Train_3,Test_3,Valid_3)\n",
        "a4=Acc_Iris(Train_4,Test_4,Valid_4)\n",
        "a5=Acc_Iris(Train_5,Test_5,Valid_5)\n",
        "a6=Acc_Iris(Train_6,Test_6,Valid_6)\n",
        "a7=Acc_Iris(Train_7,Test_7,Valid_7)\n",
        "a8=Acc_Iris(Train_8,Test_8,Valid_8)\n",
        "a9=Acc_Iris(Train_9,Test_9,Valid_9)\n",
        "a10=Acc_Iris(Train_10,Test_10,Valid_10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/series.py:856: RuntimeWarning: overflow encountered in exp\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3kTaXMDud_f",
        "colab_type": "code",
        "outputId": "25ad6caa-00c0-47f9-fec1-4a3ddcad3e74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#the mean and standard deviation of the 8 cross-validation accuracies. \n",
        "ACC_valid=[a1[0],a2[0],a3[0],a4[0],a5[0],a6[0],a7[0],a8[0],a9[0],a10[0]]\n",
        "\n",
        "MEAN_ACC1=np.mean(ACC_valid)\n",
        "STD_ACC1=np.std(ACC_valid)\n",
        "\n",
        "print('MEAN v =',MEAN_ACC1)\n",
        "print('STANDARD DEVIATION =',STD_ACC1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN v = 0.8\n",
            "STANDARD DEVIATION = 0.1520233900132184\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O9ZuWS4yIQz",
        "colab_type": "code",
        "outputId": "4ecbeb60-7334-45e5-8343-90fe218b63f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# the accuracy on the test-set.\n",
        "ACC_test=[a1[1],a2[1],a3[1],a4[1],a5[1],a6[1],a7[1],a8[1],a9[1],a10[1]]\n",
        "\n",
        "MEAN_ACC2=np.mean(ACC_test)\n",
        "STD_ACC2=np.std(ACC_test)\n",
        "\n",
        "print('MEAN v =',MEAN_ACC2)\n",
        "print('STANDARD DEVIATION =',STD_ACC2)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN v = 0.8100000000000002\n",
            "STANDARD DEVIATION = 0.1556705923844749\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}